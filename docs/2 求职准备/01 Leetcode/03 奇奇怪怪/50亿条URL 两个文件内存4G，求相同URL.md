> created: 2022-05-24T11:27:46
> tags: []
> source: [原文地址](https://www.cnblogs.com/aspirant/p/7154551.html)
> author: aspirant
            关注 - 21
            粉丝 - 1094

---

# 1. 给定 a、b 两个文件，各存放 50 亿个 url，每个 url 各占 64 字节，内存限制是 4G，让你找出 a、b 文件共同的 url?

假如每个 url 大小为 10bytes，那么可以估计每个文件的大小为 50G×64=320G，远远大于内存限制的 4G，所以不可能将其完全加载到内存中处理，可以采用分治的思想来解决。

　　Step1：遍历文件 a，对每个 url 求取 hash(url)%1000，然后根据所取得的值将 url 分别存储到 1000 个小文件(记为 a0,a1,...,a999，每个小文件约 300M);

　　Step2:遍历文件 b，采取和 a 相同的方式将 url 分别存储到 1000 个小文件(记为 b0,b1,...,b999);

　　巧妙之处：这样处理后，所有可能相同的 url 都被保存在对应的小文件(a0vsb0,a1vsb1,...,a999vsb999)中，不对应的小文件不可能有相同的 url。然后我们只要求出这个 1000 对小文件中相同的 url 即可。

　　Step3：求每对小文件 ai 和 bi 中相同的 url 时，可以把 ai 的 url 存储到 hash\_set/hash\_map 中。然后遍历 bi 的每个 url，看其是否在刚才构建的 hash\_set 中，如果是，那么就是共同的 url，存到文件里面就可以了。

　　草图如下(左边分解 A，右边分解 B，中间求解相同 url)：

![](https://pic4.zhimg.com/v2-de57d897ce478ff55f85a98fb0f1beff_b.png)

# 2.有一个 1G 大小的一个文件，里面每一行是一个词，词的大小不超过 16 字节，内存限制大小是 1M，要求返回频数最高的 100 个词。

　　Step1：顺序读文件中，对于每个词 x，取 hash(x)%5000，然后按照该值存到 5000 个小文件(记为 f0,f1,...,f4999)中，这样每个文件大概是 200k 左右，如果其中的有的文件超过了 1M 大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过 1M;

　　Step2：对每个小文件，统计每个文件中出现的词以及相应的频率(可以采用 trie 树/hash\_map 等)，并取出出现频率最大的 100 个词(可以用含 100 个结点的最小堆)，并把 100 词及相应的频率存入文件，这样又得到了 5000 个文件;

　　Step3：把这 5000 个文件进行归并(类似与归并排序);

　　草图如下(分割大问题，求解小问题，归并)：

　草图如下(分割大问题，求解小问题，归并)：

# 3.现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个 IP。

　　Step1：从这一天的日志数据中把访问百度的 IP 取出来，逐个写入到一个大文件中;

　　Step2：注意到 IP 是 32 位的，最多有 2^32 个 IP。同样可以采用映射的方法，比如模 1000，把整个大文件映射为 1000 个小文件;

　　Step3：找出每个小文中出现频率最大的 IP(可以采用 hash\_map 进行频率统计，然后再找出频率最大的几个)及相应的频率;

　　Step4：在这 1000 个最大的 IP 中，找出那个频率最大的 IP，即为所求。

　　草图如下：

![](https://pic1.zhimg.com/v2-e7be8c1ac929d114e4b8a8583ee12274_b.png)
